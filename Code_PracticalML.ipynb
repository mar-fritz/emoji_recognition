{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c35bbd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mediapipe in /home/mfritz/.local/lib/python3.6/site-packages (0.8.3)\n",
      "Requirement already satisfied: opencv-python in /home/mfritz/.local/lib/python3.6/site-packages (4.5.2.54)\n",
      "Requirement already satisfied: pandas in /home/mfritz/.local/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: scikit-learn in /home/mfritz/.local/lib/python3.6/site-packages (0.24.2)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from mediapipe) (0.30.0)\n",
      "Requirement already satisfied: attrs in /home/mfritz/.local/lib/python3.6/site-packages (from mediapipe) (21.2.0)\n",
      "Requirement already satisfied: protobuf>=3.11.4 in /home/mfritz/.local/lib/python3.6/site-packages (from mediapipe) (3.17.3)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from mediapipe) (1.11.0)\n",
      "Requirement already satisfied: numpy==1.19.3 in /home/mfritz/.local/lib/python3.6/site-packages (from mediapipe) (1.19.3)\n",
      "Requirement already satisfied: absl-py in /home/mfritz/.local/lib/python3.6/site-packages (from mediapipe) (0.12.0)\n",
      "Requirement already satisfied: dataclasses in /home/mfritz/.local/lib/python3.6/site-packages (from mediapipe) (0.8)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/mfritz/.local/lib/python3.6/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/mfritz/.local/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/mfritz/.local/lib/python3.6/site-packages (from scikit-learn) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/mfritz/.local/lib/python3.6/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/mfritz/.local/lib/python3.6/site-packages (from scikit-learn) (1.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a5ddfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in /home/mfritz/.local/lib/python3.6/site-packages (1.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f638038a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (5.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0edc6b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp # Import mediapipe\n",
    "import cv2 # Import opencv\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import accuracy_score # Accuracy metrics \n",
    "import pickle \n",
    "\n",
    "import emoji\n",
    "from PIL import ImageFont, ImageDraw, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c7b87e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c98a86",
   "metadata": {},
   "source": [
    "# Get Data from files and combine to one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79901065",
   "metadata": {},
   "outputs": [],
   "source": [
    "maria_df = pd.read_csv('data/coords_maria.csv')\n",
    "konna_df = pd.read_csv('data/konna_data.csv')\n",
    "elena_df = pd.read_csv('data/elena_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f735008",
   "metadata": {},
   "outputs": [],
   "source": [
    "konna_df['class'] = konna_df['class'].replace({'neutral': 'neutral_face'})\n",
    "elena_df['class'] = elena_df['class'].replace({'neutral': 'neutral_face'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef0705ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "face_screaming_in_fear     786\n",
       "neutral_face               714\n",
       "smiling_face_with_halo     632\n",
       "smiling_face_with_horns    595\n",
       "slightly_smiling_face      593\n",
       "grinning_face              570\n",
       "hugging_face               512\n",
       "shushing_face              484\n",
       "white_frowning_face        403\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maria_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baba3854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grinning_face              731\n",
       "smiling_face_with_halo     659\n",
       "slightly_smiling_face      622\n",
       "neutral_face               617\n",
       "hugging_face               586\n",
       "white_frowning_face        581\n",
       "face_screaming_in_fear     546\n",
       "smiling_face_with_horns    450\n",
       "shushing_face              450\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "konna_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3e49107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smiling_face_with_halo     750\n",
       "grinning_face              705\n",
       "slightly_smiling_face      631\n",
       "neutral_face               629\n",
       "smiling_face_with_horns    608\n",
       "face_screaming_in_fear     605\n",
       "hugging_face               541\n",
       "shushing_face              500\n",
       "white_frowning_face        479\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elena_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9b7225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = maria_df.append(konna_df, ignore_index=True).append(elena_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b65a469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smiling_face_with_halo     2041\n",
       "grinning_face              2006\n",
       "neutral_face               1960\n",
       "face_screaming_in_fear     1937\n",
       "slightly_smiling_face      1846\n",
       "smiling_face_with_horns    1653\n",
       "hugging_face               1639\n",
       "white_frowning_face        1463\n",
       "shushing_face              1434\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca351bb",
   "metadata": {},
   "source": [
    "## Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ba39390",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_final.drop('class', axis=1) # features\n",
    "y = df_final['class'] # target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dedbb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0635a656",
   "metadata": {},
   "source": [
    "# Train Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f236b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'lr':make_pipeline(StandardScaler(), LogisticRegression(max_iter=10000)),\n",
    "    'svm':make_pipeline(StandardScaler(), SVC(probability=True)),\n",
    "    'rf':make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=10)),\n",
    "    'lr_pca':make_pipeline(StandardScaler(), PCA(n_components = 50), LogisticRegression(max_iter=10000)),\n",
    "    'svm_pca':make_pipeline(StandardScaler(), PCA(n_components = 50), SVC(probability=True)),\n",
    "    'rf_pca':make_pipeline(StandardScaler(), PCA(n_components = 50), RandomForestClassifier(max_depth=10)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8cfbd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    fit_models[algo] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff1e72dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('logisticregression', LogisticRegression(max_iter=10000))]),\n",
       " 'svm': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('svc', SVC(probability=True))]),\n",
       " 'rf': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('randomforestclassifier',\n",
       "                  RandomForestClassifier(max_depth=10))]),\n",
       " 'lr_pca': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('pca', PCA(n_components=50)),\n",
       "                 ('logisticregression', LogisticRegression(max_iter=10000))]),\n",
       " 'svm_pca': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('pca', PCA(n_components=50)), ('svc', SVC(probability=True))]),\n",
       " 'rf_pca': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                 ('pca', PCA(n_components=50)),\n",
       "                 ('randomforestclassifier',\n",
       "                  RandomForestClassifier(max_depth=10))])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffaa28d",
   "metadata": {},
   "source": [
    "# Test Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61b6b74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.9987484355444305\n",
      "svm 0.9874843554443054\n",
      "rf 0.9899874843554443\n",
      "lr_pca 0.9974968710888611\n",
      "svm_pca 0.9870671672924489\n",
      "rf_pca 0.9941593658740092\n"
     ]
    }
   ],
   "source": [
    "for algo, model in fit_models.items():\n",
    "    yhat = model.predict(X_test)\n",
    "    print(algo, accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728102f",
   "metadata": {},
   "source": [
    "# Save models in pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0b51112",
   "metadata": {},
   "outputs": [],
   "source": [
    "for algo, model in fit_models.items():\n",
    "    with open(algo+'.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d5340",
   "metadata": {},
   "source": [
    "# Make Detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b2471",
   "metadata": {},
   "source": [
    "Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5de8853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'svm.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d84175d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0cee4de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(probability=True))])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231487db",
   "metadata": {},
   "source": [
    "Detections every frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "460e4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        # change video fps\n",
    "        cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Export coordinates\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "            \n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "            \n",
    "            # Concate rows\n",
    "            row = pose_row+face_row\n",
    "            \n",
    "            # Make Detections\n",
    "            X = pd.DataFrame([row])\n",
    "            body_language_class = model.predict(X)[0]\n",
    "            body_language_prob = model.predict_proba(X)[0]\n",
    "            #print(body_language_class, np.round(body_language_prob, decimals=1))\n",
    "                               \n",
    "            # Get status box\n",
    "            cv2.rectangle(image, (0,0), (460, 60), (245, 117, 16), -1)\n",
    "\n",
    "            # Display Class\n",
    "            cv2.putText(image, 'CLASS'\n",
    "                        , (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, body_language_class.split(' ')[0]\n",
    "                        , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Display Probability\n",
    "            cv2.putText(image, 'PROB'\n",
    "                        , (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(round(body_language_prob[np.argmax(body_language_prob)],2))\n",
    "                        , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "            # add emoji to frame\n",
    "            # load with alpha channel -1\n",
    "            emoticon = cv2.imread(body_language_class+'.png', -1)\n",
    "            img_height, img_width, _ = emoticon.shape\n",
    "            x_offset = 0\n",
    "            y_offset = 60\n",
    "            y1, y2 = y_offset, y_offset + img_height\n",
    "            x1, x2 = x_offset, x_offset + img_width\n",
    "            # image[ y1:y2 , x1:x2 ] = emoticon\n",
    "            alpha_s = emoticon[:, :, 3] / 255.0\n",
    "            alpha_l = 1.0 - alpha_s\n",
    "\n",
    "            for c in range(0, 3):\n",
    "                image[y1:y2, x1:x2, c] = (alpha_s * emoticon[:, :, c] +\n",
    "                                          alpha_l * image[y1:y2, x1:x2, c])\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "                        \n",
    "        cv2.imshow('Emoji Recognition', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77df512",
   "metadata": {},
   "source": [
    "Detections with sampling (class is updated every time the blue box flashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e20ef603",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    time = 0\n",
    "    predictions = []\n",
    "    agg_pred = ' '\n",
    "    while cap.isOpened():\n",
    "        # change video fps\n",
    "        cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "        # get frame\n",
    "        ret, frame = cap.read()\n",
    "        # recolor feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # make detections\n",
    "        results = holistic.process(image)\n",
    "        \n",
    "        # recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # export coordinates\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "            \n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "            \n",
    "            # Concate rows\n",
    "            row = pose_row+face_row\n",
    "            \n",
    "            # Make Detections\n",
    "            X = pd.DataFrame([row])\n",
    "            body_language_class = model.predict(X)[0]\n",
    "            body_Savelanguage_prob = model.predict_proba(X)[0]\n",
    "            \n",
    "            predictions.append(body_language_class)\n",
    "            \n",
    "            \n",
    "            if time % 10 == 0:\n",
    "                agg_pred = max(set(predictions), key = predictions.count)\n",
    "                predictions = []\n",
    "                time = 0\n",
    "                # Get status box\n",
    "                cv2.rectangle(image, (0,0), (460, 60), (245, 117, 16), -1)\n",
    "            # Display Class\n",
    "            cv2.putText(image, 'CLASS', (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, agg_pred.split(' ')[0]\n",
    "                            , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            # add emoji to frame\n",
    "            # load with alpha channel -1\n",
    "            emoticon = cv2.imread(agg_pred+'.png', -1)\n",
    "            img_height, img_width, _ = emoticon.shape\n",
    "            x_offset = 0\n",
    "            y_offset = 60\n",
    "            y1, y2 = y_offset, y_offset + img_height\n",
    "            x1, x2 = x_offset, x_offset + img_width\n",
    "            # image[ y1:y2 , x1:x2 ] = emoticon\n",
    "            alpha_s = emoticon[:, :, 3] / 255.0\n",
    "            alpha_l = 1.0 - alpha_s\n",
    "\n",
    "            for c in range(0, 3):\n",
    "                image[y1:y2, x1:x2, c] = (alpha_s * emoticon[:, :, c] +\n",
    "                                          alpha_l * image[y1:y2, x1:x2, c])\n",
    "            time += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        cv2.imshow('Emoji Recognition', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6cc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicalml",
   "language": "python",
   "name": "practicalml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
